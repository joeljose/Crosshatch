{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosshatch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/joeljose/Crosshatch/blob/main/crosshatch.ipynb)\n",
    "\n",
    "Turn portrait photos into crosshatch drawings using **SAM2** for segmentation and hatch pattern blending.\n",
    "\n",
    "This notebook walks through the full pipeline step by step, then gives you a one-call function to process your own images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Crosshatching?\n",
    "\n",
    "Crosshatching is the drawing of two layers of hatching at right-angles to create a mesh-like pattern. Multiple layers in varying directions can be used to create textures. Crosshatching is often used to create tonal effects, by varying the spacing of lines or by adding additional layers of lines. Crosshatching is used in pencil drawing, but is particularly useful with pen and ink drawing, to create the impression of areas of tone, since the pen can only create a solid black line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Walkthrough\n",
    "\n",
    "Let us look at the process of creating a crosshatch drawing.\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land0.jpg)\n",
    "\n",
    "First we draw the edges and contours we see in our photo:\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land1.jpg)\n",
    "\n",
    "In crosshatching, we create dark regions by drawing multiple hatches on those areas. Lighter areas contain progressively fewer hatches:\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land2.jpg)\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land3.jpg)\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land3.5.jpg)\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land3.7.jpg)\n",
    "\n",
    "And the finished drawing:\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Approach\n",
    "\n",
    "We imitate the steps above in code:\n",
    "\n",
    "1. **Segment** — isolate the subject from the background using SAM2\n",
    "2. **Resize** — scale the image to match the hatch texture dimensions\n",
    "3. **Layer** — place the subject on a white background\n",
    "4. **Histogram** — analyze the tonal distribution of the image\n",
    "5. **Threshold** — split the tonal range into three equal-area zones\n",
    "6. **Hatch** — map a different line texture onto each zone\n",
    "7. **Blend** — combine the hatch layers into the final drawing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Colab only downloads the notebook file, not the rest of the repo.\n# Clone so we have the assets/ directory (textures, sample images, etc.)\nif not os.path.exists('Crosshatch'):\n    !git clone -q https://github.com/joeljose/Crosshatch.git\nos.chdir('Crosshatch')\n\n# Install PyTorch (CPU)\n!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cpu\n\n# Install other dependencies\n!pip install -q opencv-python-headless matplotlib numpy pillow"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install SAM2 (cloned as sam2_repo to avoid shadowing the installed package)\nif not os.path.exists('sam2_repo'):\n    !git clone -q https://github.com/facebookresearch/sam2.git sam2_repo\n    !pip install -q -e sam2_repo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download SAM2 checkpoint\ncheckpoint_dir = 'sam2_repo/checkpoints'\ncheckpoint_file = f'{checkpoint_dir}/sam2.1_hiera_small.pt'\n\nif not os.path.exists(checkpoint_file):\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    !wget -q -O {checkpoint_file} https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt\n    print(f'Downloaded checkpoint to {checkpoint_file}')\nelse:\n    print('Checkpoint already downloaded')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load SAM2 Model\n",
    "\n",
    "[SAM2](https://github.com/facebookresearch/sam2) (Segment Anything Model 2) by Meta can segment any object in an image given a point prompt. We use it to isolate the portrait subject from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "checkpoint_path = 'sam2_repo/checkpoints/sam2.1_hiera_small.pt'\nconfig_path = 'configs/sam2.1/sam2.1_hiera_s.yaml'\n\nmodel = build_sam2(config_path, checkpoint_path, device='cpu')\npredictor = SAM2ImagePredictor(model)\n\nprint('SAM2 model loaded')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_person(image, predictor, center_point=None):\n",
    "    \"\"\"Segment the main subject using SAM2 with a center-point prompt.\"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    h, w = image_rgb.shape[:2]\n",
    "    if center_point is None:\n",
    "        center_point = (w // 2, h // 2)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        predictor.set_image(image_rgb)\n",
    "        masks, scores, _ = predictor.predict(\n",
    "            point_coords=np.array([[center_point[0], center_point[1]]]),\n",
    "            point_labels=np.array([1]),\n",
    "            multimask_output=True,\n",
    "        )\n",
    "\n",
    "    best_mask = masks[np.argmax(scores)]\n",
    "    return (best_mask * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def calculate_thresholds(image):\n",
    "    \"\"\"Split the tonal range into three equal-area zones.\n",
    "\n",
    "    Returns the pixel values at the 25th, 50th, and 75th percentile\n",
    "    of the non-white pixels.\n",
    "    \"\"\"\n",
    "    counts, _ = np.histogram(image, bins=256, range=(0, 256))\n",
    "    total = np.sum(counts[:255])  # exclude white background\n",
    "\n",
    "    thresholds = []\n",
    "    cum_sum = 0\n",
    "    for percentile in (0.25, 0.50, 0.75):\n",
    "        target = total * percentile\n",
    "        for i in range(255):\n",
    "            cum_sum += counts[i]\n",
    "            if cum_sum > target:\n",
    "                thresholds.append(i)\n",
    "                break\n",
    "\n",
    "    return tuple(thresholds)\n",
    "\n",
    "\n",
    "def blend_images(images):\n",
    "    \"\"\"Blend a list of images with equal weight.\"\"\"\n",
    "    fraction = 1.0 / len(images)\n",
    "    output = np.zeros_like(images[0], dtype=np.float32)\n",
    "    for img in images:\n",
    "        output += img.astype(np.float32) * fraction\n",
    "    return output.astype(np.uint8)\n",
    "\n",
    "\n",
    "def show_images(images, titles, figsize=(15, 5)):\n",
    "    \"\"\"Display images side by side.\"\"\"\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Hatch Textures\n",
    "\n",
    "We use four hand-drawn hatch patterns:\n",
    "\n",
    "| Texture | Purpose |\n",
    "|---|---|\n",
    "| **right** | Diagonal lines (\\\\) — darkest tones |\n",
    "| **left** | Diagonal lines (/) — mid tones |\n",
    "| **horizontal** | Horizontal lines — lightest tones (classic style) |\n",
    "| **vortex** | Circular swirl — lightest tones (artistic style) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = cv2.imread('assets/textures/leftx.png', 0)\n",
    "right = cv2.imread('assets/textures/rightx.png', 0)\n",
    "horizontal = cv2.imread('assets/textures/horizontalx.png', 0)\n",
    "vortex = cv2.imread('assets/textures/vortexx.png', 0)\n",
    "\n",
    "print(f'Texture dimensions: {left.shape}')\n",
    "\n",
    "show_images(\n",
    "    [right, left, horizontal, vortex],\n",
    "    ['Right (darkest)', 'Left (mid)', 'Horizontal (lightest)', 'Vortex (alt lightest)'],\n",
    "    figsize=(20, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Sample Portrait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'assets/samples/lena.png'\n",
    "\n",
    "image_color = cv2.imread(image_path)\n",
    "image_gray = cv2.imread(image_path, 0)\n",
    "\n",
    "print(f'Image size: {image_gray.shape}')\n",
    "plt.imshow(cv2.cvtColor(image_color, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 — Segment the Subject\n",
    "\n",
    "SAM2 takes a single point prompt (the image center) and returns a binary mask separating the subject from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = segment_person(image_color, predictor)\n",
    "\n",
    "show_images(\n",
    "    [cv2.cvtColor(image_color, cv2.COLOR_BGR2RGB), mask],\n",
    "    ['Original', 'Segmentation Mask'],\n",
    "    figsize=(12, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Resize\n",
    "\n",
    "The hatch textures are 2100 px. We resize the portrait so that its largest side is 1200 px — this produces visually appealing hatch line density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_unit = 1200\n",
    "hatch_unit = 2100\n",
    "\n",
    "height, width = image_gray.shape\n",
    "ratio = max_unit / max(width, height)\n",
    "new_width = int(ratio * width)\n",
    "new_height = int(ratio * height)\n",
    "\n",
    "image_resized = cv2.resize(image_gray, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)\n",
    "mask_resized = cv2.resize(mask, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "print(f'Resized to {new_width} x {new_height}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 — Layer on White Background\n",
    "\n",
    "We place the segmented subject on a pure-white canvas. This ensures the background doesn't interfere with the hatching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = np.ones_like(image_resized) * 255\n",
    "layered = np.where(mask_resized == 255, image_resized, background)\n",
    "\n",
    "plt.imshow(layered, cmap='gray')\n",
    "plt.title('Layered (subject on white)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 — Analyze Tonal Range\n",
    "\n",
    "We plot the histogram of the layered image and find three threshold values that divide the non-white pixel area into equal thirds. Each third will receive a different hatch texture.\n",
    "\n",
    "![histogram thresholds](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/hist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "counts, bins = np.histogram(layered, range(257))\n",
    "plt.bar(bins[:-1] - 0.5, counts, width=1, edgecolor='none')\n",
    "plt.xlim([-0.5, 265.5])\n",
    "plt.xlabel('Pixel intensity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of layered image')\n",
    "plt.show()\n",
    "\n",
    "# Calculate thresholds\n",
    "thresh1, thresh2, thresh3 = calculate_thresholds(layered)\n",
    "print(f'Thresholds: {thresh1}, {thresh2}, {thresh3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 — Apply Hatch Patterns\n",
    "\n",
    "Each hatch layer covers one tonal zone:\n",
    "\n",
    "| Layer | Texture | Tonal zone |\n",
    "|---|---|---|\n",
    "| hatch 1 | right (\\\\) | darkest — pixels below threshold 1 |\n",
    "| hatch 2 | left (/) | mid — pixels below threshold 2 |\n",
    "| hatch 3 | horizontal or vortex | lightest — pixels below threshold 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop textures to match the resized image\n",
    "left_crop = left[:new_height, :new_width]\n",
    "right_crop = right[:new_height, :new_width]\n",
    "horizontal_crop = horizontal[:new_height, :new_width]\n",
    "\n",
    "# Vortex needs a center crop\n",
    "start_y = (hatch_unit - new_height) // 2\n",
    "start_x = (hatch_unit - new_width) // 2\n",
    "vortex_crop = vortex[start_y:start_y + new_height, start_x:start_x + new_width]\n",
    "\n",
    "# Build hatch layers (using horizontal for the third layer)\n",
    "hatch1 = np.where(layered < thresh1, right_crop, background)\n",
    "hatch2 = np.where(layered < thresh2, left_crop, background)\n",
    "hatch3 = np.where(layered < thresh3, horizontal_crop, background)\n",
    "\n",
    "show_images(\n",
    "    [hatch1, hatch2, hatch3],\n",
    "    ['Hatch 1 (right — darkest)', 'Hatch 2 (left — mid)', 'Hatch 3 (horizontal — lightest)'],\n",
    "    figsize=(18, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 — Blend\n",
    "\n",
    "All three hatch layers are blended with equal weight to produce the final crosshatch drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = blend_images([hatch1, hatch2, hatch3])\n",
    "\n",
    "show_images(\n",
    "    [cv2.cvtColor(image_color, cv2.COLOR_BGR2RGB), output],\n",
    "    ['Original', 'Crosshatch (horizontal)'],\n",
    "    figsize=(14, 6),\n",
    ")\n",
    "\n",
    "cv2.imwrite('output_horizontal.jpg', output)\n",
    "print('Saved output_horizontal.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Vortex Style\n",
    "\n",
    "Swapping the third hatch layer from horizontal lines to a circular vortex pattern gives a more artistic look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatch3_vortex = np.where(layered < thresh3, vortex_crop, background)\n",
    "output_vortex = blend_images([hatch1, hatch2, hatch3_vortex])\n",
    "\n",
    "show_images(\n",
    "    [output, output_vortex],\n",
    "    ['Horizontal style', 'Vortex style'],\n",
    "    figsize=(14, 6),\n",
    ")\n",
    "\n",
    "cv2.imwrite('output_vortex.jpg', output_vortex)\n",
    "print('Saved output_vortex.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete Function\n",
    "\n",
    "Everything above wrapped into a single reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crosshatch(image_path, output_path='output.jpg', hatch_style='horizontal',\n",
    "                      max_dimension=1200):\n",
    "    \"\"\"Create a crosshatch drawing from a portrait image.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the input image.\n",
    "        output_path: Where to save the result.\n",
    "        hatch_style: 'horizontal' or 'vortex'.\n",
    "        max_dimension: Resize the longest side to this value.\n",
    "\n",
    "    Returns:\n",
    "        The crosshatched image as a numpy array.\n",
    "    \"\"\"\n",
    "    hatch_unit = 2100\n",
    "\n",
    "    # Load\n",
    "    img_color = cv2.imread(image_path)\n",
    "    img_gray = cv2.imread(image_path, 0)\n",
    "    h, w = img_gray.shape\n",
    "\n",
    "    # Segment\n",
    "    mask = segment_person(img_color, predictor)\n",
    "\n",
    "    # Resize\n",
    "    r = max_dimension / max(w, h)\n",
    "    nw, nh = int(r * w), int(r * h)\n",
    "    img_resized = cv2.resize(img_gray, (nw, nh), interpolation=cv2.INTER_LANCZOS4)\n",
    "    mask_resized = cv2.resize(mask, (nw, nh), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    # Layer on white\n",
    "    bg = np.ones_like(img_resized) * 255\n",
    "    layered = np.where(mask_resized == 255, img_resized, bg)\n",
    "\n",
    "    # Thresholds\n",
    "    t1, t2, t3 = calculate_thresholds(layered)\n",
    "\n",
    "    # Crop textures\n",
    "    l_crop = left[:nh, :nw]\n",
    "    r_crop = right[:nh, :nw]\n",
    "    h_crop = horizontal[:nh, :nw]\n",
    "    sy = (hatch_unit - nh) // 2\n",
    "    sx = (hatch_unit - nw) // 2\n",
    "    v_crop = vortex[sy:sy + nh, sx:sx + nw]\n",
    "\n",
    "    # Hatch layers\n",
    "    h1 = np.where(layered < t1, r_crop, bg)\n",
    "    h2 = np.where(layered < t2, l_crop, bg)\n",
    "    third = v_crop if hatch_style == 'vortex' else h_crop\n",
    "    h3 = np.where(layered < t3, third, bg)\n",
    "\n",
    "    # Blend and save\n",
    "    result = blend_images([h1, h2, h3])\n",
    "    cv2.imwrite(output_path, result)\n",
    "    return result\n",
    "\n",
    "\n",
    "print('create_crosshatch() defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Try Your Own Image\n",
    "\n",
    "Upload a portrait and generate a crosshatch drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print('Upload your portrait image:')\n",
    "uploaded = files.upload()\n",
    "uploaded_filename = list(uploaded.keys())[0]\n",
    "print(f'Uploaded: {uploaded_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = create_crosshatch(\n",
    "    uploaded_filename,\n",
    "    'my_crosshatch.jpg',\n",
    "    hatch_style='horizontal',  # change to 'vortex' for a different look\n",
    ")\n",
    "\n",
    "# Show original vs result\n",
    "original_rgb = cv2.cvtColor(cv2.imread(uploaded_filename), cv2.COLOR_BGR2RGB)\n",
    "show_images(\n",
    "    [original_rgb, result],\n",
    "    ['Original', 'Crosshatch'],\n",
    "    figsize=(14, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('my_crosshatch.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tips\n",
    "\n",
    "- **Centered subject** — SAM2 uses the image center as a point prompt, so the subject should be roughly centered.\n",
    "- **Good contrast** — Images with clear lighting and contrast produce the best hatching.\n",
    "- **Style choice** — Use `'horizontal'` for a classic look and `'vortex'` for something more artistic.\n",
    "- **Resolution** — The `max_dimension` parameter controls output size. Larger values preserve more detail but take longer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}