{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Crosshatch\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/joeljose/Crosshatch/blob/main/crosshatch.ipynb)\n\nTurn portrait photos into crosshatch drawings using automatic segmentation and hatch pattern blending.\n\nThis notebook walks through the full pipeline step by step, then gives you a one-call function to process your own images."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Crosshatching?\n",
    "\n",
    "Crosshatching is the drawing of two layers of hatching at right-angles to create a mesh-like pattern. Multiple layers in varying directions can be used to create textures. Crosshatching is often used to create tonal effects, by varying the spacing of lines or by adding additional layers of lines. Crosshatching is used in pencil drawing, but is particularly useful with pen and ink drawing, to create the impression of areas of tone, since the pen can only create a solid black line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Walkthrough\n",
    "\n",
    "Let us look at the process of creating a crosshatch drawing.\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land0.jpg)\n",
    "\n",
    "First we draw the edges and contours we see in our photo:\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land1.jpg)\n",
    "\n",
    "In crosshatching, we create dark regions by drawing multiple hatches on those areas. Lighter areas contain progressively fewer hatches:\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land2.jpg)\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land3.jpg)\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land3.5.jpg)\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land3.7.jpg)\n",
    "\n",
    "And the finished drawing:\n",
    "\n",
    "![image](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/land4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Our Approach\n\nWe imitate the steps above in code:\n\n1. **Segment** — isolate the subject from the background using rembg\n2. **Resize** — scale the image to match the hatch texture dimensions\n3. **Layer** — place the subject on a white background\n4. **Histogram** — analyze the tonal distribution of the image\n5. **Threshold** — split the tonal range into three equal-area zones\n6. **Hatch** — map a different line texture onto each zone\n7. **Blend** — combine the hatch layers into the final drawing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Colab only downloads the notebook file, not the rest of the repo.\n# Clone so we have the assets/ directory (textures, sample images, etc.)\nif not os.path.exists('Crosshatch'):\n    !git clone -q https://github.com/joeljose/Crosshatch.git\nos.chdir('Crosshatch')\n\n# Pin numpy to Colab's pre-installed version to avoid breaking other packages\nimport importlib.metadata as _md\n_np_ver = _md.version('numpy')\n!pip install -q \"rembg[cpu]\" \"numpy=={_np_ver}\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom rembg import new_session, remove\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (12, 8)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Load Segmentation Model\n\n[rembg](https://github.com/danielgatis/rembg) removes the background from images using the `u2net_human_seg` model, which is optimized for human subjects. The model is downloaded automatically on first use."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "session = new_session(\"u2net_human_seg\")\nprint('Segmentation model loaded')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Helper Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def segment_person(image, session):\n    \"\"\"Segment the main subject using rembg.\"\"\"\n    if len(image.shape) == 2:\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    else:\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    pil_image = Image.fromarray(image_rgb)\n    mask_pil = remove(pil_image, session=session, only_mask=True)\n    mask = np.array(mask_pil)\n    mask = np.where(mask > 128, 255, 0).astype(np.uint8)\n    return mask\n\n\ndef calculate_thresholds(image):\n    \"\"\"Split the tonal range into three equal-area zones.\n\n    Returns the pixel values at the 25th, 50th, and 75th percentile\n    of the non-white pixels.\n    \"\"\"\n    counts, _ = np.histogram(image, bins=256, range=(0, 256))\n    total = np.sum(counts[:255])  # exclude white background\n\n    thresholds = []\n    cum_sum = 0\n    targets = [total * p for p in (0.25, 0.50, 0.75)]\n\n    for i in range(255):\n        cum_sum += counts[i]\n        if targets and cum_sum > targets[0]:\n            thresholds.append(i)\n            targets.pop(0)\n\n    return tuple(thresholds)\n\n\ndef blend_images(images):\n    \"\"\"Blend a list of images with equal weight.\"\"\"\n    fraction = 1.0 / len(images)\n    output = np.zeros_like(images[0], dtype=np.float32)\n    for img in images:\n        output += img.astype(np.float32) * fraction\n    return output.astype(np.uint8)\n\n\ndef show_images(images, titles, figsize=(15, 5)):\n    \"\"\"Display images side by side.\"\"\"\n    n = len(images)\n    fig, axes = plt.subplots(1, n, figsize=figsize)\n    if n == 1:\n        axes = [axes]\n    for ax, img, title in zip(axes, images, titles):\n        ax.imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n        ax.set_title(title)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Hatch Textures\n",
    "\n",
    "We use four hand-drawn hatch patterns:\n",
    "\n",
    "| Texture | Purpose |\n",
    "|---|---|\n",
    "| **right** | Diagonal lines (\\\\) — darkest tones |\n",
    "| **left** | Diagonal lines (/) — mid tones |\n",
    "| **horizontal** | Horizontal lines — lightest tones (classic style) |\n",
    "| **vortex** | Circular swirl — lightest tones (artistic style) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "left = cv2.imread('assets/textures/leftx.png', 0)\nright = cv2.imread('assets/textures/rightx.png', 0)\nhorizontal = cv2.imread('assets/textures/horizontalx.png', 0)\nvortex = cv2.imread('assets/textures/vortexx.png', 0)\n\nprint(f'Texture dimensions: {left.shape}')\n\nshow_images(\n    [right, left, horizontal, vortex],\n    ['Right (darkest)', 'Left (mid)', 'Horizontal (lightest)', 'Vortex (alt lightest)'],\n    figsize=(20, 5),\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Sample Portrait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'assets/samples/lena.png'\n",
    "\n",
    "image_color = cv2.imread(image_path)\n",
    "image_gray = cv2.imread(image_path, 0)\n",
    "\n",
    "print(f'Image size: {image_gray.shape}')\n",
    "plt.imshow(cv2.cvtColor(image_color, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 1 — Segment the Subject\n\nrembg automatically detects and segments the subject, returning a binary mask that separates the person from the background."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "mask = segment_person(image_color, session)\n\nshow_images(\n    [cv2.cvtColor(image_color, cv2.COLOR_BGR2RGB), mask],\n    ['Original', 'Segmentation Mask'],\n    figsize=(12, 5),\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2 — Resize\n\nThe hatch textures are 2100 px. We resize the portrait so that its largest side is 1200 px — this produces visually appealing hatch line density."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "max_unit = 1200\nhatch_unit = 2100\n\nheight, width = image_gray.shape\nratio = max_unit / max(width, height)\nnew_width = int(ratio * width)\nnew_height = int(ratio * height)\n\nimage_resized = cv2.resize(image_gray, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)\nmask_resized = cv2.resize(mask, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)\n\nprint(f'Resized to {new_width} x {new_height}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 — Layer on White Background\n",
    "\n",
    "We place the segmented subject on a pure-white canvas. This ensures the background doesn't interfere with the hatching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = np.ones_like(image_resized) * 255\n",
    "layered = np.where(mask_resized == 255, image_resized, background)\n",
    "\n",
    "plt.imshow(layered, cmap='gray')\n",
    "plt.title('Layered (subject on white)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 — Analyze Tonal Range\n",
    "\n",
    "We plot the histogram of the layered image and find three threshold values that divide the non-white pixel area into equal thirds. Each third will receive a different hatch texture.\n",
    "\n",
    "![histogram thresholds](https://github.com/joeljose/Crosshatch/raw/main/assets/tutorial/hist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "counts, bins = np.histogram(layered, range(257))\n",
    "plt.bar(bins[:-1] - 0.5, counts, width=1, edgecolor='none')\n",
    "plt.xlim([-0.5, 265.5])\n",
    "plt.xlabel('Pixel intensity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of layered image')\n",
    "plt.show()\n",
    "\n",
    "# Calculate thresholds\n",
    "thresh1, thresh2, thresh3 = calculate_thresholds(layered)\n",
    "print(f'Thresholds: {thresh1}, {thresh2}, {thresh3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 — Apply Hatch Patterns\n",
    "\n",
    "Each hatch layer covers one tonal zone:\n",
    "\n",
    "| Layer | Texture | Tonal zone |\n",
    "|---|---|---|\n",
    "| hatch 1 | right (\\\\) | darkest — pixels below threshold 1 |\n",
    "| hatch 2 | left (/) | mid — pixels below threshold 2 |\n",
    "| hatch 3 | horizontal or vortex | lightest — pixels below threshold 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop textures to match the resized image\n",
    "left_crop = left[:new_height, :new_width]\n",
    "right_crop = right[:new_height, :new_width]\n",
    "horizontal_crop = horizontal[:new_height, :new_width]\n",
    "\n",
    "# Vortex needs a center crop\n",
    "start_y = (hatch_unit - new_height) // 2\n",
    "start_x = (hatch_unit - new_width) // 2\n",
    "vortex_crop = vortex[start_y:start_y + new_height, start_x:start_x + new_width]\n",
    "\n",
    "# Build hatch layers (using horizontal for the third layer)\n",
    "hatch1 = np.where(layered < thresh1, right_crop, background)\n",
    "hatch2 = np.where(layered < thresh2, left_crop, background)\n",
    "hatch3 = np.where(layered < thresh3, horizontal_crop, background)\n",
    "\n",
    "show_images(\n",
    "    [hatch1, hatch2, hatch3],\n",
    "    ['Hatch 1 (right — darkest)', 'Hatch 2 (left — mid)', 'Hatch 3 (horizontal — lightest)'],\n",
    "    figsize=(18, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 — Blend\n",
    "\n",
    "All three hatch layers are blended with equal weight to produce the final crosshatch drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = blend_images([hatch1, hatch2, hatch3])\n",
    "\n",
    "show_images(\n",
    "    [cv2.cvtColor(image_color, cv2.COLOR_BGR2RGB), output],\n",
    "    ['Original', 'Crosshatch (horizontal)'],\n",
    "    figsize=(14, 6),\n",
    ")\n",
    "\n",
    "cv2.imwrite('output_horizontal.jpg', output)\n",
    "print('Saved output_horizontal.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Vortex Style\n",
    "\n",
    "Swapping the third hatch layer from horizontal lines to a circular vortex pattern gives a more artistic look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatch3_vortex = np.where(layered < thresh3, vortex_crop, background)\n",
    "output_vortex = blend_images([hatch1, hatch2, hatch3_vortex])\n",
    "\n",
    "show_images(\n",
    "    [output, output_vortex],\n",
    "    ['Horizontal style', 'Vortex style'],\n",
    "    figsize=(14, 6),\n",
    ")\n",
    "\n",
    "cv2.imwrite('output_vortex.jpg', output_vortex)\n",
    "print('Saved output_vortex.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete Function\n",
    "\n",
    "Everything above wrapped into a single reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_crosshatch(image_path, output_path='output.jpg', hatch_style='horizontal',\n                      max_dimension=1200):\n    \"\"\"Create a crosshatch drawing from a portrait image.\n\n    Args:\n        image_path: Path to the input image.\n        output_path: Where to save the result.\n        hatch_style: 'horizontal' or 'vortex'.\n        max_dimension: Resize the longest side to this value.\n\n    Returns:\n        The crosshatched image as a numpy array.\n    \"\"\"\n    hatch_unit = 2100\n\n    # Load\n    img_color = cv2.imread(image_path)\n    img_gray = cv2.imread(image_path, 0)\n    h, w = img_gray.shape\n\n    # Segment\n    mask = segment_person(img_color, session)\n\n    # Resize\n    r = max_dimension / max(w, h)\n    nw, nh = int(r * w), int(r * h)\n    img_resized = cv2.resize(img_gray, (nw, nh), interpolation=cv2.INTER_LANCZOS4)\n    mask_resized = cv2.resize(mask, (nw, nh), interpolation=cv2.INTER_LANCZOS4)\n\n    # Layer on white\n    bg = np.ones_like(img_resized) * 255\n    layered = np.where(mask_resized == 255, img_resized, bg)\n\n    # Thresholds\n    t1, t2, t3 = calculate_thresholds(layered)\n\n    # Crop textures\n    l_crop = left[:nh, :nw]\n    r_crop = right[:nh, :nw]\n    h_crop = horizontal[:nh, :nw]\n    sy = (hatch_unit - nh) // 2\n    sx = (hatch_unit - nw) // 2\n    v_crop = vortex[sy:sy + nh, sx:sx + nw]\n\n    # Hatch layers\n    h1 = np.where(layered < t1, r_crop, bg)\n    h2 = np.where(layered < t2, l_crop, bg)\n    third = v_crop if hatch_style == 'vortex' else h_crop\n    h3 = np.where(layered < t3, third, bg)\n\n    # Blend and save\n    result = blend_images([h1, h2, h3])\n    cv2.imwrite(output_path, result)\n    return result\n\n\nprint('create_crosshatch() defined')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Try Your Own Image\n\n1. Click the **folder icon** in the left sidebar to open the file browser\n2. Navigate into the **Crosshatch** folder\n3. **Upload** your portrait image there (drag and drop or right-click > Upload)\n4. **Edit the filename** in the cell below to match your uploaded file\n5. Run the cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "uploaded_filename = 'my_portrait.jpg'  # <-- change this to your uploaded filename\n\nresult = create_crosshatch(\n    uploaded_filename,\n    'my_crosshatch.jpg',\n    hatch_style='horizontal',  # change to 'vortex' for a different look\n)\n\n# Show original vs result\noriginal_rgb = cv2.cvtColor(cv2.imread(uploaded_filename), cv2.COLOR_BGR2RGB)\nshow_images(\n    [original_rgb, result],\n    ['Original', 'Crosshatch'],\n    figsize=(14, 6),\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('my_crosshatch.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Tips\n\n- **Good contrast** — Images with clear lighting and contrast produce the best hatching.\n- **Style choice** — Use `'horizontal'` for a classic look and `'vortex'` for something more artistic.\n- **Resolution** — The `max_dimension` parameter controls output size. Larger values preserve more detail but take longer."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}