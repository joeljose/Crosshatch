{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "crosshatch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_hzldYeRbuP"
      },
      "source": [
        "# Crosshatching\n",
        "Crosshatching is the drawing of two layers of hatching at right-angles to create a mesh-like pattern. Multiple layers in varying directions can be used to create textures. Crosshatching is often used to create tonal effects, by varying the spacing of lines or by adding additional layers of lines. Crosshatching is used in pencil drawing, but is particularly useful with pen and ink drawing, to create the impression of areas of tone, since the pen can only create a solid black line. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3XBQkyHkFQV"
      },
      "source": [
        "Let us look at the process of creating a crosshatch drawing.</br>\n",
        "![image](https://github.com/joeljose/assets/raw/master/crosshatch/land0.jpg)\n",
        "</br>First we draw the edges/contours we see in our photo</br>\n",
        "![image](https://github.com/joeljose/assets/raw/master/crosshatch/land1.jpg)\n",
        "</br>In crosshatching, we create dark regions by drawing multiple hatches on those areas. And lighter areas contain progressively lesser number of hatches</br>\n",
        "![image](https://github.com/joeljose/assets/raw/master/crosshatch/land2.jpg)</br>\n",
        "![image](https://github.com/joeljose/assets/raw/master/crosshatch/land3.jpg)</br>\n",
        "![image](https://github.com/joeljose/assets/raw/master/crosshatch/land3.5.jpg)</br>\n",
        "![image](https://github.com/joeljose/assets/raw/master/crosshatch/land3.7.jpg)\n",
        "</br>And finally we get </br>\n",
        "![image](https://github.com/joeljose/assets/raw/master/crosshatch/land4.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcaAFQjwlYq9"
      },
      "source": [
        "In this project, we are trying to imitate the steps the artist did but through code, to produce crosshatching effect on portraits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxUDPTu0_wlV"
      },
      "source": [
        "## All essential imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0QZnAL9_c80"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "import cv2\n",
        "from six.moves import urllib\n",
        "from copy import deepcopy\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import Image as IMG\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1wisq2eAPJ6"
      },
      "source": [
        "## Import helper methods\n",
        "These methods help us perform the following tasks:\n",
        "* Load the latest version of the pretrained DeepLab model\n",
        "* Load the colormap from the PASCAL VOC dataset\n",
        "* Adds colors to various labels, such as \"pink\" for people, \"green\" for bicycle and more\n",
        "* Visualize an image, and add an overlay of colors on various regions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdJOW-uA_lC4"
      },
      "source": [
        "class DeepLabModel(object):\n",
        "  \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
        "\n",
        "  INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
        "  OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
        "  INPUT_SIZE = 513\n",
        "  FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "  def __init__(self, tarball_path):\n",
        "    \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
        "    self.graph = tf.Graph()\n",
        "\n",
        "    graph_def = None\n",
        "    # Extract frozen graph from tar archive.\n",
        "    tar_file = tarfile.open(tarball_path)\n",
        "    for tar_info in tar_file.getmembers():\n",
        "      if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "        file_handle = tar_file.extractfile(tar_info)\n",
        "        graph_def = tf.GraphDef.FromString(file_handle.read())\n",
        "        break\n",
        "\n",
        "    tar_file.close()\n",
        "\n",
        "    if graph_def is None:\n",
        "      raise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "\n",
        "    with self.graph.as_default():\n",
        "      tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "    self.sess = tf.Session(graph=self.graph)\n",
        "\n",
        "  def run(self, image):\n",
        "    \"\"\"Runs inference on a single image.\n",
        "\n",
        "    Args:\n",
        "      image: A PIL.Image object, raw input image.\n",
        "\n",
        "    Returns:\n",
        "      resized_image: RGB image resized from original input image.\n",
        "      seg_map: Segmentation map of `resized_image`.\n",
        "    \"\"\"\n",
        "    width, height = image.size\n",
        "    resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
        "    print(width, height)\n",
        "    print(\"Resize Ratio - {}\".format(resize_ratio))\n",
        "    target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
        "    print(target_size)\n",
        "    # target_size = (width, height)\n",
        "    resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "    batch_seg_map = self.sess.run(\n",
        "        self.OUTPUT_TENSOR_NAME,\n",
        "        feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "    seg_map = batch_seg_map[0]\n",
        "    return resized_image, seg_map\n",
        "\n",
        "\n",
        "def create_pascal_label_colormap():\n",
        "  \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n",
        "\n",
        "  Returns:\n",
        "    A Colormap for visualizing segmentation results.\n",
        "  \"\"\"\n",
        "  colormap = np.zeros((256, 3), dtype=int)\n",
        "  ind = np.arange(256, dtype=int)\n",
        "\n",
        "  for shift in reversed(range(8)):\n",
        "    for channel in range(3):\n",
        "      colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
        "    ind >>= 3\n",
        "\n",
        "  return colormap\n",
        "\n",
        "\n",
        "def label_to_color_image(label):\n",
        "  \"\"\"Adds color defined by the dataset colormap to the label.\n",
        "\n",
        "  Args:\n",
        "    label: A 2D array with integer type, storing the segmentation label.\n",
        "\n",
        "  Returns:\n",
        "    result: A 2D array with floating type. The element of the array\n",
        "      is the color indexed by the corresponding element in the input label\n",
        "      to the PASCAL color map.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If label is not of rank 2 or its value is larger than color\n",
        "      map maximum entry.\n",
        "  \"\"\"\n",
        "  if label.ndim != 2:\n",
        "    raise ValueError('Expect 2-D input label')\n",
        "\n",
        "  colormap = create_pascal_label_colormap()\n",
        "\n",
        "  if np.max(label) >= len(colormap):\n",
        "    raise ValueError('label value too large.')\n",
        "\n",
        "  return colormap[label]\n",
        "\n",
        "\n",
        "def vis_segmentation(image, seg_map):\n",
        "  \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
        "\n",
        "  plt.subplot(grid_spec[0])\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.title('input image')\n",
        "\n",
        "  plt.subplot(grid_spec[1])\n",
        "  seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
        "  plt.imshow(seg_image)\n",
        "  plt.axis('off')\n",
        "  plt.title('segmentation map')\n",
        "\n",
        "  plt.subplot(grid_spec[2])\n",
        "  plt.imshow(image)\n",
        "  plt.imshow(seg_image, alpha=0.7)\n",
        "  plt.axis('off')\n",
        "  plt.title('segmentation overlay')\n",
        "\n",
        "  unique_labels = np.unique(seg_map)\n",
        "  ax = plt.subplot(grid_spec[3])\n",
        "  plt.imshow(\n",
        "      FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "  ax.yaxis.tick_right()\n",
        "  plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
        "  plt.xticks([], [])\n",
        "  ax.tick_params(width=0.0)\n",
        "  plt.grid('off')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5l50i1wcIyr"
      },
      "source": [
        "LABEL_NAMES = np.asarray([\n",
        "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
        "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
        "]) \n",
        "\n",
        "#  All category of things our model predicts\n",
        "\n",
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9tIjKKp_1dJ"
      },
      "source": [
        "MODEL_NAME = 'mobilenetv2_coco_voctrainaug'  # @param ['mobilenetv2_coco_voctrainaug', 'mobilenetv2_coco_voctrainval', 'xception_coco_voctrainaug', 'xception_coco_voctrainval']\n",
        "\n",
        "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "_MODEL_URLS = {\n",
        "    'mobilenetv2_coco_voctrainaug':\n",
        "        'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
        "    'mobilenetv2_coco_voctrainval':\n",
        "        'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',\n",
        "    'xception_coco_voctrainaug':\n",
        "        'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n",
        "    'xception_coco_voctrainval':\n",
        "        'deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n",
        "}\n",
        "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
        "\n",
        "model_dir = tempfile.mkdtemp()\n",
        "tf.gfile.MakeDirs(model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
        "print('downloading model, this might take a while...')\n",
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME],\n",
        "                   download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZPGGQ9kJJEr"
      },
      "source": [
        "def run_visualization(IMAGE_NAME):\n",
        "  \"\"\"Inferences DeepLab model and visualizes result.\"\"\"\n",
        "  try:\n",
        "    original_im = Image.open(IMAGE_NAME).convert('L')\n",
        "  except IOError:\n",
        "    print('Cannot retrieve image. Please check url: ' + url)\n",
        "    return\n",
        "\n",
        "  print('running deeplab on image')\n",
        "  resized_im, seg_map = MODEL.run(original_im)\n",
        "  vis_segmentation(resized_im, seg_map)\n",
        "  return resized_im, seg_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3iC9vBy_4_Z"
      },
      "source": [
        "def run_without_visualization(IMAGE_NAME):\n",
        "  \"\"\"Inferences DeepLab model and visualizes result.\"\"\"\n",
        "  try:\n",
        "    original_im = Image.open(IMAGE_NAME).convert('L')\n",
        "  except IOError:\n",
        "    print('Cannot retrieve image. Please check url: ' + url)\n",
        "    return\n",
        "\n",
        "  print('running deeplab on image')\n",
        "  resized_im, seg_map = MODEL.run(original_im)\n",
        "  return resized_im, seg_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JyDWW5XW-Yz"
      },
      "source": [
        "## helper method to download files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgGUo08jW5Tv"
      },
      "source": [
        "def download_file(url, dest_filename):\n",
        "    \"\"\"Downloads the file in given url\"\"\"\n",
        "    if os.path.isfile(dest_filename):\n",
        "        print('Already Downloaded: %s to %s' % (url, dest_filename))\n",
        "        return\n",
        "    print('Downloading: %s to %s' % (url, dest_filename))\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "    if not response.ok:\n",
        "        raise Exception(\"Couldn't download file\")\n",
        "\n",
        "    with open(dest_filename, 'wb') as fp:\n",
        "        for block in response.iter_content(1024):\n",
        "            fp.write(block)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZBma97YXFJL"
      },
      "source": [
        "## Downloading and loading Hatch images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k-KcuS-XSLS"
      },
      "source": [
        "download_file(\"https://github.com/joeljose/assets/raw/master/crosshatch/horizontalx.png\",\"horizontal.png\")\n",
        "download_file(\"https://github.com/joeljose/assets/raw/master/crosshatch/leftx.png\",\"left.png\")\n",
        "download_file(\"https://github.com/joeljose/assets/raw/master/crosshatch/rightx.png\",\"right.png\")\n",
        "download_file(\"https://github.com/joeljose/assets/raw/master/crosshatch/vortexx.png\",\"vortex.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKmE8ZJRHCBD"
      },
      "source": [
        "left=cv2.imread(\"left.png\",0)\n",
        "right=cv2.imread(\"right.png\",0)\n",
        "vortex=cv2.imread(\"vortex.png\",0)\n",
        "horizontal=cv2.imread(\"horizontal.png\",0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crmsHy3QaGCp"
      },
      "source": [
        "## Downloading face image that we want to apply crosshatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDsC_DRcZ2kw"
      },
      "source": [
        "download_file(\"https://github.com/joeljose/assets/raw/master/crosshatch/niiu.jpeg\",\"face.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSwJEueR8cQR"
      },
      "source": [
        "We apply the model to our image, and it gives back a segmentation map. We then use this segmentation map to remove the background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_XqGk-TCPFo"
      },
      "source": [
        "IMAGE_NAME = 'face.jpg'\n",
        "resized_im, seg_map = run_visualization(IMAGE_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8I3q27OFIrt"
      },
      "source": [
        "## We only need the 'person' class from our segmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqbuouSFJIUc"
      },
      "source": [
        "LABEL_NAMES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w97NbFG2CJNb"
      },
      "source": [
        "LABEL_NAMES[15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBxoMwhNFwkp"
      },
      "source": [
        "The model creates an image called segmap which labels all pixels which are classified as \"person\" as 15. </br>\n",
        "So let's create a new binary image that labels the background as 0(black), and person as 255(white)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOo_wALQEcw-"
      },
      "source": [
        "mapping = np.zeros([seg_map.shape[0],seg_map.shape[1]])\n",
        "mapping[seg_map != 15] = 0\n",
        "mapping[seg_map == 15] = 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY_U1lJEFmvM"
      },
      "source": [
        "# Resize face image and segmentation map to our required size\n",
        "I found out through trial and error that the hatch images are perfect(visually appealing) when we resize the input face image to a certain resolution. Resizing the hatch images and fitting it to the face image also works, but the hatches look ugly(in my opinion). You are welcome to try it if you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRuI_tgqEfA-"
      },
      "source": [
        "face = cv2.imread(IMAGE_NAME,0)\n",
        "\n",
        "height, width = face.shape\n",
        "\n",
        "max_unit = 1200\n",
        "hatch_unit = 2100\n",
        "face_unit =max(width,height)\n",
        "ratio=max_unit/face_unit\n",
        "new_height=int(ratio*height)\n",
        "new_width=int(ratio*width)\n",
        "face_resized = cv2.resize(face,(new_width,new_height),Image.ANTIALIAS)\n",
        "mapping_resized = cv2.resize(mapping,(new_width,new_height),Image.ANTIALIAS)\n",
        "plt.imshow(mapping_resized,cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRU0LnwWHejF"
      },
      "source": [
        "# Applying Layers\n",
        "We use the segmentation map to make a new image which has white background and the person of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42pvO86d98sz"
      },
      "source": [
        "back_img=255*np.ones(face_resized.shape) # white image\n",
        "\n",
        "layered_image= np.where(mapping_resized == 255, \n",
        "                         face_resized, \n",
        "                         back_img)\n",
        "plt.imshow(layered_image,cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eMUfA-AuqH1"
      },
      "source": [
        "## Histogram \n",
        "We plot the histogram of our image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3_j0XkP_XJ8"
      },
      "source": [
        "counts, bins = np.histogram(layered_image, range(257))\n",
        "plt.bar(bins[:-1] - 0.5, counts, width=1, edgecolor='none')\n",
        "plt.xlim([-0.5, 265.5])\n",
        "plt.ylim([-0.5, 20000])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwttYJI6J5yv"
      },
      "source": [
        "We need to find 3 threshold values as shown in the diagram such that the areas under the histogram are equal. We will ignore the final impulse in the histogram, since it is just the white background(value=255). \n",
        "![image](https://github.com/joeljose/assets/raw/master/crosshatch/hist.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1-LUPuEJiOp"
      },
      "source": [
        "total=np.sum(counts[:255]) # we exclude the last white value. \n",
        "\n",
        "\n",
        "def find_thresh(val):\n",
        "  cum_sum=0\n",
        "  for i in range(255):\n",
        "    cum_sum+=counts[i]\n",
        "    if cum_sum>(val):\n",
        "      return i\n",
        "      \n",
        "thresh1,thresh2,thresh3=find_thresh(total*0.25),find_thresh(total/2),find_thresh(total*0.75)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T3jPN2xL9lw"
      },
      "source": [
        "## Cropping out the hatch images in the dimensions of our face image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmX29DsFDi1A"
      },
      "source": [
        "\n",
        "left_new=left[:new_height,:new_width]\n",
        "right_new=right[:new_height,:new_width]\n",
        "horizontal_new=horizontal[:new_height,:new_width]\n",
        "# cropping out the center of the vortex\n",
        "vortex_new=vortex[((hatch_unit//2)-(new_height//2)):((hatch_unit//2)-(new_height//2)+new_height),\n",
        "              ((hatch_unit//2)-(new_width//2)):((hatch_unit//2)-(new_width//2)+new_width)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti4SJtj8zZlX"
      },
      "source": [
        "## hatch1 image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "072agVFOKaQv"
      },
      "source": [
        "hatch1 = np.where(layered_image<thresh1,right_new,back_img)\n",
        "plt.imshow(hatch1, cmap=\"gray\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUQlO-fGzeIs"
      },
      "source": [
        "## hatch2 image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDxwdM98Oa84"
      },
      "source": [
        "hatch2 = np.where(layered_image<thresh2,left_new,back_img)\n",
        "plt.imshow(hatch2, cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlHweW_CzgNY"
      },
      "source": [
        "## hatch3 image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5dJOlm2O-jb"
      },
      "source": [
        "hatch3 = np.where(layered_image<thresh3,horizontal_new,back_img)\n",
        "#you can apply vortex effect if you want. Just comment the top one and uncomment the bottom one.\n",
        "# hatch3 = np.where(layered_image<thresh3,vortex_new,back_img)   \n",
        "plt.imshow(hatch3, cmap=\"gray\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwXhTVy4o7uV"
      },
      "source": [
        "## Now we need to blend in all three images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAzUGolePNqO"
      },
      "source": [
        "def blend(list_images): # Blend images equally.\n",
        "\n",
        "    equal_fraction = 1.0 / (len(list_images))\n",
        "\n",
        "    output = np.zeros_like(list_images[0])\n",
        "\n",
        "    for img in list_images:\n",
        "        output = output + img * equal_fraction\n",
        "\n",
        "    output = output.astype(np.uint8)\n",
        "    return output\n",
        "\n",
        "list_images = [hatch1, hatch2, hatch3]\n",
        "output = blend(list_images)\n",
        "\n",
        "plt.imshow(output,cmap=\"gray\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFETmTJlP4qd"
      },
      "source": [
        "cv2.imwrite(\"output.jpg\", output)\n",
        "IMG(\"output.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9dRqucb0VEc"
      },
      "source": [
        "Now let us try to integrate all we have done into a single method that takes in an image and outputs the crosshatch image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrw58KDk1lQK"
      },
      "source": [
        "def hatching(IMAGE_NAME,apply_vortex=False):\n",
        "\n",
        "  resized_im, seg_map = run_without_visualization(IMAGE_NAME)\n",
        "\n",
        "  mapping = np.zeros([seg_map.shape[0],seg_map.shape[1]])\n",
        "  mapping[seg_map != 15] = 0\n",
        "  mapping[seg_map == 15] = 255\n",
        "\n",
        "  face = cv2.imread(IMAGE_NAME,0)\n",
        "  height, width = face.shape\n",
        "\n",
        "  max_unit = 1200\n",
        "  hatch_unit = 2100\n",
        "\n",
        "  face_unit =max(width,height)\n",
        "  ratio=max_unit/face_unit\n",
        "  new_height=int(ratio*height)\n",
        "  new_width=int(ratio*width)\n",
        "\n",
        "  face_resized = cv2.resize(face,(new_width,new_height),Image.ANTIALIAS)\n",
        "  mapping_resized = cv2.resize(mapping,(new_width,new_height),Image.ANTIALIAS)\n",
        "\n",
        "  back_img=255*np.ones(face_resized.shape) # white image\n",
        "\n",
        "  layered_image= np.where(mapping_resized == 255, \n",
        "                          face_resized, \n",
        "                          back_img)\n",
        "  \n",
        "  counts, bins = np.histogram(layered_image, range(257))\n",
        "\n",
        "  total=np.sum(counts[:255]) # we exclude the last white value.     \n",
        "  thresh1,thresh2,thresh3=find_thresh(total*0.25),find_thresh(total/2),find_thresh(total*0.75)\n",
        "\n",
        "  left_new = left[:new_height,:new_width]\n",
        "  right_new = right[:new_height,:new_width]\n",
        "  horizontal_new = horizontal[:new_height,:new_width]\n",
        "\n",
        "  # cropping out the center of the vortex\n",
        "  vortex_new=vortex[((hatch_unit//2)-(new_height//2)):((hatch_unit//2)-(new_height//2)+new_height),\n",
        "                ((hatch_unit//2)-(new_width//2)):((hatch_unit//2)-(new_width//2)+new_width)]\n",
        "\n",
        "  hatch1 = np.where(layered_image<thresh1,right_new,back_img)\n",
        "  hatch2 = np.where(layered_image<thresh2,left_new,back_img)\n",
        "  if apply_vortex:\n",
        "    hatch3 = np.where(layered_image<thresh3,vortex_new,back_img)\n",
        "  else:\n",
        "    hatch3 = np.where(layered_image<thresh3,horizontal_new,back_img)\n",
        "\n",
        "  list_images = [hatch1, hatch2, hatch3]\n",
        "  output = blend(list_images)\n",
        "\n",
        "  cv2.imwrite(\"new_output.jpg\", output)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilQi137G4WKs"
      },
      "source": [
        "Lets try the vortex effect on a new image.\n",
        "If you want to try your own image, upload it and give it as IMAGE_NAME argument to the 'hatching' method. Also skip the next code unit, where i just download a new image for trying out the vortex effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMBWpf0b3ECx"
      },
      "source": [
        "download_file(\"https://github.com/joeljose/assets/raw/master/crosshatch/me.jpg\",\"newface.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__j9NxXI5vm2"
      },
      "source": [
        "hatching(\"newface.jpg\",True)\n",
        "IMG(\"new_output.jpg\") "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}